{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-1-1d6f60b255b6>:12: DeprecationWarning: The variable qiskit.aqua.aqua_globals is deprecated. It was moved/refactored to qiskit.utils.algorithm_globals (pip install qiskit-terra). For more information see <https://github.com/Qiskit/qiskit-aqua/blob/master/README.md#migration-guide>\n  aqua_globals.random_seed = seed\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from qiskit import BasicAer\n",
    "from qiskit.circuit.library import ZZFeatureMap, ZFeatureMap, PauliFeatureMap\n",
    "\n",
    "from qiskit.aqua import QuantumInstance, aqua_globals\n",
    "from qiskit.aqua.algorithms import QSVM, VQC\n",
    "from qiskit.aqua.utils import split_dataset_to_data_and_labels, map_label_to_class_name\n",
    "\n",
    "seed = 10599\n",
    "aqua_globals.random_seed = seed\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.decomposition import PCA\n",
    "main_folder=str(Path.cwd().parent) \n",
    "sys.path.append(main_folder) \n",
    "data_folder = f'{main_folder}/data'\n",
    "\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qiskit import IBMQ\n",
    "\n",
    "# # IBMQ.save_account(TOKEN)\n",
    "# IBMQ.load_account() # Load account from disk\n",
    "# IBMQ.providers()    # List all available providers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # choose backend\n",
    "# provider = IBMQ.get_provider(\"ibm-q\")\n",
    "\n",
    "# for backend in provider.backends():\n",
    "#   try:\n",
    "#     qubit_count = len(backend.properties().qubits)\n",
    "#   except:\n",
    "#     qubit_count = \"simulated\"\n",
    "\n",
    "#   print(f\"{backend.name()} has {backend.status().pending_jobs} queued and {qubit_count} qubits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO:\n",
    "\n",
    "- Improve the accuracy via Feature Engineer or Hyperparam\n",
    "- Try with 3 classes (STAR, GALAXY , QSOR)\n",
    "- Try with more data (currently training is 100 and test is 40 and test extra is 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the processed data DR16_Processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_balanced(data, target, train_size, test_size):\n",
    "    \n",
    "    np.random.seed(0)\n",
    "\n",
    "    classes = np.unique(target)\n",
    "    # can give test_size as fraction of input data size of number of samples\n",
    "    if test_size<1:\n",
    "        n_test = np.round(len(target)*test_size)\n",
    "    else:\n",
    "        n_test = test_size\n",
    "    n_train = train_size #max(0,len(target)-n_test)\n",
    "    n_train_per_class = max(1,int(np.floor(n_train/len(classes))))\n",
    "    n_test_per_class = max(1,int(np.floor(n_test/len(classes))))\n",
    "\n",
    "    ixs = []\n",
    "    for cl in classes:\n",
    "        if (n_train_per_class+n_test_per_class) > np.sum(target==cl):\n",
    "            # if data has too few samples for this class, do upsampling\n",
    "            # split the data to training and testing before sampling so data points won't be\n",
    "            #  shared among training and test data\n",
    "            splitix = int(np.ceil(n_train_per_class/(n_train_per_class+n_test_per_class)*np.sum(target==cl)))\n",
    "            ixs.append(np.r_[np.random.choice(np.nonzero(target==cl)[0][:splitix], n_train_per_class),\n",
    "                np.random.choice(np.nonzero(target==cl)[0][splitix:], n_test_per_class)])\n",
    "        else:\n",
    "            ixs.append(np.random.choice(np.nonzero(target==cl)[0], n_train_per_class+n_test_per_class,\n",
    "                replace=False))\n",
    "\n",
    "    # take same num of samples from all classes\n",
    "    ix_train = np.concatenate([x[:n_train_per_class] for x in ixs])\n",
    "    ix_test = np.concatenate([x[n_train_per_class:(n_train_per_class+n_test_per_class)] for x in ixs])\n",
    "\n",
    "    X_train = data[ix_train,:]\n",
    "    X_test = data[ix_test,:]\n",
    "    y_train = target[ix_train]\n",
    "    y_test = target[ix_test]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     ┌───┐┌──────────────┐                                           ┌───┐»\nq_0: ┤ H ├┤ U1(2.0*x[0]) ├──■─────────────────────────────────────■──┤ H ├»\n     ├───┤├──────────────┤┌─┴─┐┌───────────────────────────────┐┌─┴─┐├───┤»\nq_1: ┤ H ├┤ U1(2.0*x[1]) ├┤ X ├┤ U1(2.0*(π - x[0])*(π - x[1])) ├┤ X ├┤ H ├»\n     └───┘└──────────────┘└───┘└───────────────────────────────┘└───┘└───┘»\n«     ┌──────────────┐                                           \n«q_0: ┤ U1(2.0*x[0]) ├──■─────────────────────────────────────■──\n«     ├──────────────┤┌─┴─┐┌───────────────────────────────┐┌─┴─┐\n«q_1: ┤ U1(2.0*x[1]) ├┤ X ├┤ U1(2.0*(π - x[0])*(π - x[1])) ├┤ X ├\n«     └──────────────┘└───┘└───────────────────────────────┘└───┘\n"
     ]
    }
   ],
   "source": [
    "prep = ZZFeatureMap(2, reps=2)\n",
    "print(prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transofrmed data \n",
    "with open(f'{data_folder}/processed/DR16_processed_X.pkl','rb') as input_file:\n",
    "    X = pickle.load(input_file)\n",
    "with open(f'{data_folder}/processed/DR16_processed_y.pkl','rb') as input_file:\n",
    "    y = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['STAR', 'STAR', 'STAR', ..., 'GALAXY', 'GALAXY', 'STAR'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_num = le.fit_transform(y.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'GALAXY', 'QSO', 'STAR'}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(100, 8)\n(40, 8)\n(20, 8)\n(100,)\n(40,)\n(20,)\n"
     ]
    }
   ],
   "source": [
    "#Create training_dataset and test_dataset with STAR and NOT_STAR\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y_num, train_size=0.001, test_size=0.0006, random_state=101, stratify=y_num)\n",
    "\n",
    "X_val = X_test[0:40]\n",
    "X_pred = X_test[40:60]\n",
    "y_val = y_test[0:40]\n",
    "y_pred = y_test[40:60]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_pred.shape)\n",
    "\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)\n",
    "print(y_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 0, 51],\n",
       "       [ 1, 11],\n",
       "       [ 2, 38]])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = 200 \n",
    "test_samples  = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(198, 8)\n(150, 8)\n(198,)\n(150,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_balanced(np.array(X), y_num, train_size=train_samples, test_size=test_samples)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Training data distribution\n [[ 0 66]\n [ 1 66]\n [ 2 66]]\n Test data distribution\n [[ 0 50]\n [ 1 50]\n [ 2 50]]\n"
     ]
    }
   ],
   "source": [
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(\" Training data distribution\\n\", frequencies)\n",
    "\n",
    "(unique, counts) = np.unique(y_test, return_counts=True)\n",
    "frequencies = np.asarray((unique, counts)).T\n",
    "print(\" Test data distribution\\n\", frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['GALAXY', 'STAR', 'QSO']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "class_labels = list(set(y))\n",
    "class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "source": [
    "## No Reduction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints = [np.array(X_test),np.array(y_test)]\n",
    "# If class labels are numeric\n",
    "training_size = len(X_train)\n",
    "test_size = len(X_test)\n",
    "\n",
    "#this is where I transform our dataframe to Dict[key:np.array]\n",
    "if class_labels[0].isdigit():\n",
    "        # Pick training size number of samples from each distro\n",
    "    training_input = {key: (X_train[y_train == int(key), :])[:training_size] for k, key in enumerate(class_labels)}\n",
    "    test_input = {key: (X_test[y_test == int(key), :])[: test_size] for k, key in enumerate(class_labels)}\n",
    "else:\n",
    "    # if they aren't\n",
    "    training_input = {key: (X_train[y_train == k, :])[:training_size] for k, key in enumerate(class_labels)}\n",
    "    test_input = {key: (X_test[y_test == k, :])[:test_size] for k, key in enumerate(class_labels)}"
   ]
  },
  {
   "source": [
    "## PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pca = PCA(n_components=3).fit(X_train)\n",
    "# X_train = pca.transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "\n",
    "# # Create datapoint[X_test,y]\n",
    "# datapoints = [np.array(X_test),np.array(y_test)]\n",
    "# # If class labels are numeric\n",
    "# training_size = len(X_train)\n",
    "# test_size = len(X_test)\n",
    "\n",
    "# #this is where I transform our dataframe to Dict[key:np.array]\n",
    "# if class_labels[0].isdigit():\n",
    "#         # Pick training size number of samples from each distro\n",
    "#     training_input = {key: (X_train[y_train == int(key), :])[:training_size] for k, key in enumerate(class_labels)}\n",
    "#     test_input = {key: (X_test[y_test == int(key), :])[: test_size] for k, key in enumerate(class_labels)}\n",
    "# else:\n",
    "#     # if they aren't\n",
    "#     training_input = {key: (X_train[y_train == k, :])[:training_size] for k, key in enumerate(class_labels)}\n",
    "#     test_input = {key: (X_test[y_test == k, :])[:test_size] for k, key in enumerate(class_labels)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "\n",
    "# total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "# print(total_var)\n",
    "\n",
    "# labels = {\n",
    "#     str(i): f\"PC {i+1} ({var:.1f}%)\"\n",
    "#     for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n",
    "# }\n",
    "\n",
    "# fig = px.scatter(X_train, x=0, y=1, color=y_train)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "# fig = px.scatter_3d(\n",
    "#     X_train, x=0, y=1, z=2, color=y_train,\n",
    "#     title=f'Total Explained Variance: {total_var:.2f}%',\n",
    "#     labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}\n",
    "# )\n",
    "# fig.show()"
   ]
  },
  {
   "source": [
    "## ISOMap"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import Isomap, LocallyLinearEmbedding\n",
    "# iso_embedding = Isomap(n_components=3, n_jobs = 4, n_neighbors = 5)\n",
    "# iso_embedding.fit(X_train[0:5000,:])\n",
    "# X_train_iso = iso_embedding.transform(X_train)\n",
    "# X_test_iso = iso_embedding.transform(X_test)\n",
    "\n",
    "# # Create datapoint[X_test_iso,y]\n",
    "# datapoints = [np.array(X_test_iso),np.array(y_test)]\n",
    "# # If class labels are numeric\n",
    "# training_size = len(X_train_iso)\n",
    "# test_size = len(X_test_iso)\n",
    "\n",
    "# #this is where I transform our dataframe to Dict[key:np.array]\n",
    "# if class_labels[0].isdigit():\n",
    "#         # Pick training size number of samples from each distro\n",
    "#     training_input_iso = {key: (X_train_iso[y_train == int(key), :])[:training_size] for k, key in enumerate(class_labels)}\n",
    "#     test_input_iso = {key: (X_test_iso[y_test == int(key), :])[: test_size] for k, key in enumerate(class_labels)}\n",
    "# else:\n",
    "#     # if they aren't\n",
    "#     training_input_iso = {key: (X_train_iso[y_train == k, :])[:training_size] for k, key in enumerate(class_labels)}\n",
    "#     test_input_iso = {key: (X_test_iso[y_test == k, :])[:test_size] for k, key in enumerate(class_labels)}\n"
   ]
  },
  {
   "source": [
    "## Modified Locally Linear Embedding\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lle_embedding = LocallyLinearEmbedding(n_components=3, n_neighbors = 10, n_jobs = 4,  random_state=2019)\n",
    "# lle_embedding.fit(X_train[:5000,:])\n",
    "# X_train_lle = lle_embedding.transform(X_train)\n",
    "# X_test_lle = lle_embedding.transform(X_test)\n",
    "\n",
    "# # Create datapoint[X_test_lle,y]\n",
    "# datapoints = [np.array(X_test_lle),np.array(y_test)]\n",
    "# # If class labels are numeric\n",
    "# training_size = len(X_train_lle)\n",
    "# test_size = len(X_test_lle)\n",
    "\n",
    "# #this is where I transform our dataframe to Dict[key:np.array]\n",
    "# if class_labels[0].isdigit():\n",
    "#         # Pick training size number of samples from each distro\n",
    "#     training_input_lle = {key: (X_train_lle[y_train == int(key), :])[:training_size] for k, key in enumerate(class_labels)}\n",
    "#     test_input_lle = {key: (X_test_lle[y_test == int(key), :])[: test_size] for k, key in enumerate(class_labels)}\n",
    "# else:\n",
    "#     # if they aren't\n",
    "#     training_input_lle = {key: (X_train_lle[y_train == k, :])[:training_size] for k, key in enumerate(class_labels)}\n",
    "#     test_input_lle = {key: (X_test_lle[y_test == k, :])[:test_size] for k, key in enumerate(class_labels)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.aqua.components.multiclass_extensions import AllPairs, OneAgainstRest, ErrorCorrectingCode\n",
    "\n",
    "seed = 10598\n",
    "feature_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/roro/Documents/QLearning/Quantstellar-QiskitHackathon21/QUANTSTELLAR-QISKIT-HACKATHON-21/venv/lib/python3.8/site-packages/qiskit/aqua/components/multiclass_extensions/multiclass_extension.py:34: DeprecationWarning: The package qiskit.aqua.components.multiclass_extensions is deprecated. For more information see <https://github.com/Qiskit/qiskit-aqua/blob/master/README.md#migration-guide>\n",
      "  warn_package('aqua.components.multiclass_extensions')\n",
      "/Users/roro/Documents/QLearning/Quantstellar-QiskitHackathon21/QUANTSTELLAR-QISKIT-HACKATHON-21/venv/lib/python3.8/site-packages/qiskit/aqua/algorithms/classifiers/qsvm/qsvm.py:102: DeprecationWarning: The package qiskit.aqua.algorithms.classifiers is deprecated. It was moved/refactored to qiskit_machine_learning.algorithms.classifiers (pip install qiskit-machine-learning). For more information see <https://github.com/Qiskit/qiskit-aqua/blob/master/README.md#migration-guide>\n",
      "  warn_package('aqua.algorithms.classifiers',\n",
      "/Users/roro/Documents/QLearning/Quantstellar-QiskitHackathon21/QUANTSTELLAR-QISKIT-HACKATHON-21/venv/lib/python3.8/site-packages/qiskit/aqua/quantum_instance.py:135: DeprecationWarning: The class qiskit.aqua.QuantumInstance is deprecated. It was moved/refactored to qiskit.utils.QuantumInstance (pip install qiskit-terra). For more information see <https://github.com/Qiskit/qiskit-aqua/blob/master/README.md#migration-guide>\n",
      "  warn_class('aqua.QuantumInstance',\n",
      "testing success ratio:  0.5\n",
      "run time for QSVM_iso is 3584.28217792511\n"
     ]
    }
   ],
   "source": [
    "# #ISO\n",
    "# feature_map = PauliFeatureMap(feature_dimension=feature_dim, reps=1, paulis = ['Z','X','ZY'])\n",
    "# qsvm = QSVM(feature_map, training_input_iso, test_input_iso, multiclass_extension = ErrorCorrectingCode())\n",
    "\n",
    "# backend = BasicAer.get_backend('qasm_simulator')\n",
    "# quantum_instance = QuantumInstance(backend, shots=1024, seed_simulator=seed, seed_transpiler=seed)\n",
    "# start_run = time.time()\n",
    "# result_ECC_zxzy_iso = qsvm.run(quantum_instance)\n",
    "# end_run = time.time()  \n",
    "# total_run_time = end_run - start_run\n",
    "# print(\"testing success ratio: \", result_ECC_zxzy_iso['testing_accuracy'])\n",
    "# print(f'run time for QSVM_iso is {total_run_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 1\n",
    "feature_map = PauliFeatureMap(feature_dimension=feature_dim, reps=1, paulis = ['Z','X','ZY'])\n",
    "qsvm = QSVM(feature_map, training_input, test_input, multiclass_extension = AllPairs())\n",
    "\n",
    "backend = BasicAer.get_backend('qasm_simulator')\n",
    "quantum_instance = QuantumInstance(backend, shots=1024, seed_simulator=seed, seed_transpiler=seed)\n",
    "start_run = time.time()\n",
    "result_AP_zxzy_noDR = qsvm.run(quantum_instance)\n",
    "end_run = time.time()  \n",
    "total_run_time = end_run - start_run\n",
    "print(\"testing success ratio: \", result_ECC_zxzy['testing_accuracy'])\n",
    "print(f'run time for QSVM without Dimension Reduction is {total_run_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "testing success ratio:  0.3933333333333333\nrun time for QSVM_lle is 4124.346127033234\n"
     ]
    }
   ],
   "source": [
    "# Trial 2\n",
    "feature_map = PauliFeatureMap(feature_dimension=feature_dim, reps=1, paulis = ['Z','X','ZY'])\n",
    "qsvm = QSVM(feature_map, training_input, test_input, multiclass_extension = ErrorCorrectingCode())\n",
    "\n",
    "backend = BasicAer.get_backend('qasm_simulator')\n",
    "quantum_instance = QuantumInstance(backend, shots=1024, seed_simulator=seed, seed_transpiler=seed)\n",
    "\n",
    "start_run = time.time()\n",
    "result_ECC_zxzy_noDR = qsvm.run(quantum_instance)\n",
    "end_run = time.time()  \n",
    "total_run_time = end_run - start_run\n",
    "print(\"testing success ratio: \", result_ECC_zxzy_noDR['testing_accuracy'])\n",
    "print(f'run time for QSVM without Dimension Reduction is {total_run_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 2\n",
    "feature_map = PauliFeatureMap(feature_dimension=feature_dim, reps=1, paulis = ['Z','X','ZY'])\n",
    "qsvm = QSVM(feature_map, training_input, test_input, multiclass_extension = OneAgainstRest())\n",
    "\n",
    "backend = BasicAer.get_backend('qasm_simulator')\n",
    "quantum_instance = QuantumInstance(backend, shots=1024, seed_simulator=seed, seed_transpiler=seed)\n",
    "\n",
    "start_run = time.time()\n",
    "result_OAR_zxzy_noDR = qsvm.run(quantum_instance)\n",
    "end_run = time.time()  \n",
    "total_run_time = end_run - start_run\n",
    "print(\"testing success ratio: \", result_ECC_zxzy['testing_accuracy'])\n",
    "print(f'run time for QSVM without Dimension Reduction is {total_run_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial 3\n",
    "\n",
    "feature_map =  PauliFeatureMap(feature_dimension=feature_dim, reps=1, paulis = ['Z', 'ZZ'])\n",
    "qsvm = QSVM(feature_map, training_input, test_input, multiclass_extension = ErrorCorrectingCode())\n",
    "\n",
    "backend = BasicAer.get_backend('qasm_simulator')\n",
    "quantum_instance = QuantumInstance(backend, shots=1024, seed_simulator=seed, seed_transpiler=seed)\n",
    "\n",
    "start_run = time.time()\n",
    "result_ECC_zxzy = qsvm.run(quantum_instance)\n",
    "end_run = time.time()  \n",
    "total_run_time = end_run - start_run\n",
    "print(\"testing success ratio: \", result_ECC_zxzy['testing_accuracy'])\n",
    "print(f'run time for QSVM without Dimension Reduction is {total_run_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from qiskit import QuantumCircuit, execute\n",
    "# from qiskit import IBMQ\n",
    "\n",
    "\n",
    "# # IBMQ.save_account(TOKEN)\n",
    "# IBMQ.load_account() # Load account from disk\n",
    "# IBMQ.providers()    # List all available providers\n",
    "\n",
    "# # choose backend\n",
    "# provider = IBMQ.get_provider(\"ibm-q\")\n",
    "\n",
    "# feature_map = PauliFeatureMap(feature_dimension=feature_dim, reps=1, paulis = ['Z','X','ZY'])\n",
    "# qsvm = QSVM(feature_map, training_input, test_input, multiclass_extension = ErrorCorrectingCode())\n",
    "\n",
    "# # Choose backend\n",
    "# backend_qasm = provider.get_backend('ibmq_qasm_simulator')\n",
    "# quantum_instance = QuantumInstance(backend_qasm, shots=1024, seed_simulator=seed, seed_transpiler=seed)\n",
    "\n",
    "# job = execute(feature_map, backend_qasm, shots = 1024)\n",
    "# results = job.result()\n",
    "# counts = results.get_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classical SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing success ratio of SVM_iso: 0.5\nrun time for SVM_iso is 0.026098012924194336\nTesting success ratio of SVM_lle: 0.33333333333333337\nrun time for SVM_lle is 0.06100583076477051\n"
     ]
    }
   ],
   "source": [
    "from qiskit.aqua.algorithms import SklearnSVM\n",
    "\n",
    "start_run = time.time()\n",
    "result_svm_noDR = SklearnSVM(training_input, test_input, multiclass_extension =  ErrorCorrectingCode()).run()\n",
    "end_run = time.time()  \n",
    "total_run_time = end_run - start_run\n",
    "print(f'Testing success ratio of SVM without Dimension Reduction {result_svm_noDR[\"testing_accuracy\"]}')\n",
    "print(f'run time for SVM without Dimension Reduction is {total_run_time}')\n",
    "\n",
    "result_svm_lle = SklearnSVM(training_input_lle , test_input_lle , multiclass_extension =  Allpair()).run()\n",
    "end_run = time.time()  \n",
    "total_run_time = end_run - start_run\n",
    "print(f'Testing success ratio of SVM_lle: {result_svm_lle [\"testing_accuracy\"]}')\n",
    "print(f'run time for SVM_lle is {total_run_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Result \n",
    "\n",
    "with open(f'{main_folder}/models/model_SVM_noDR_200.pkl','wb') as output_file:\n",
    "     pickle.dump(result_svm_noDR,output_file)\n",
    "with open(f'{main_folder}/models/model_QSVM_noDR_200.pkl','wb') as output_file:\n",
    "     pickle.dump(result_qsvm_noDR,output_file)\n",
    "\n",
    "with open(f'{main_folder}/models/model_SVM_noDR_200.pkl','wb') as output_file:\n",
    "     pickle.dump(result_svm_noDR,output_file)\n",
    "with open(f'{main_folder}/models/model_QSVM_noDR_200.pkl','wb') as output_file:\n",
    "     pickle.dump(result_qsvm_noDR,output_file)\n",
    "\n",
    "with open(f'{main_folder}/models/model_SVM_noDR_200.pkl','wb') as output_file:\n",
    "     pickle.dump(result_svm_noDR,output_file)\n",
    "with open(f'{main_folder}/models/model_QSVM_noDR_200.pkl','wb') as output_file:\n",
    "     pickle.dump(result_qsvm_noDR,output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd00d53adc9b3e6306b4419b6c14cab3dbc6aa1f3237ea044c22b454e79edb906fd",
   "display_name": "Python 3.8.5 64-bit ('venv')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}